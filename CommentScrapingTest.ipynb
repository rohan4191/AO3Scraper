{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1191c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "def get_single_comment(comment, parent):\n",
    "\t# Find username\n",
    "\tif comment.find('h4', class_='heading byline').find('a'):\n",
    "\t\tusername = comment.find('h4', class_='heading byline').find('a').contents[0]\n",
    "\telse:\n",
    "\t\tusername = comment.find('h4', class_='heading byline').contents[0]\n",
    "\n",
    "\t# Get datetime\n",
    "\tdatetime = comment.find('h4', class_='heading byline').find('span', class_=\"posted datetime\").contents\n",
    "\tremove = ['\\n', ' ']\n",
    "\tdateObj = {}\n",
    "\n",
    "\tfor item in datetime:\n",
    "\t\tif item not in remove:\n",
    "\t\t\titemClass = item['class'][0]\n",
    "\t\t\titemValue = item.contents[0]\n",
    "\t\t\tdateObj[itemClass] = itemValue\n",
    "\n",
    "\t# Get comment id\n",
    "\tcommentid = comment['id'].split('_')[1]\n",
    "\n",
    "\t# Get direct comment text\n",
    "\ttext = comment.find('blockquote', class_ ='userstuff')#.findAll('p').contents[0]\n",
    "\ttext = text.findAll('p')\n",
    "\ttext = [item.contents[0] for item in text]\n",
    "\tfullText = ''\n",
    "\n",
    "\tfor i, item in enumerate(text):\n",
    "\t\tif i < len(text) - 1:\n",
    "\t\t\tfullText += str(item) + '\\n'\n",
    "\t\telse:\n",
    "\n",
    "\t\t\tfullText += str(item)\n",
    "\n",
    "\t# Create object and return\n",
    "\tcommentData = {'user': username, 'datetime': dateObj, 'id': commentid, 'parent': parent, 'text': fullText}\n",
    "\treturn commentData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffb1aa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment_thread(comment_thread, parent):\n",
    "\tall_comments = []\n",
    "\n",
    "\t# Recurse till you find the deepest level\n",
    "\ttry:\n",
    "\t\tnest_level = comment_thread[0]\n",
    "\t\tnest_level = nest_level.findChild('ol', class_ ='thread')[0]\n",
    "\n",
    "\t\tnew_comments = get_comment_thread(comment_thread, False)\n",
    "\t\tall_comments.append(new_comments)\n",
    "\texcept:\n",
    "\t\t# you've found the deepest level\n",
    "\t\tcomments = comment_thread[0].find_all('li')\n",
    "\t\tall_comments = []\n",
    "\n",
    "\t\t# extracts all comments in line thread\n",
    "\t\tfor c, comment in enumerate(comments):\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tif comment.attrs['class']:\n",
    "\t\t\t\t\t\tif 'odd' in comment.attrs['class'] or 'even' in comment.attrs['class']:\n",
    "\t\t\t\t\t\t\tsingle_comment = get_single_comment(comment, False)\n",
    "\t\t\t\t\t\t\tall_comments.append(single_comment)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tpass\n",
    "    \n",
    "\t\treturn all_comments\n",
    "\n",
    "\treturn all_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e56b4870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(url, header_info):\n",
    "\tall_comments = []\n",
    "\theaders = {'user-agent' : header_info}\n",
    "\n",
    "\treq = requests.get(url, headers=headers)\n",
    "\tsrc = req.text\n",
    "\tsoup = BeautifulSoup(src, 'html.parser')\n",
    "\n",
    "\t# find all pages\n",
    "\tif (soup.find('ol', class_='pagination actions')):\n",
    "\t\tpages = soup.find('ol', class_='pagination actions').findChildren(\"li\" , recursive=False)\n",
    "\t\tmax_pages = int(pages[-2].contents[0].contents[0])\n",
    "\t\tcount = 1\n",
    "\n",
    "\t\twhile count <= max_pages:\n",
    "\t\t\tcomments = soup.find('ol', class_ = 'thread').findChildren(\"li\" , recursive=False)\n",
    "\n",
    "\t\t\t# comments processing\n",
    "\t\t\tfor c, comment in enumerate(comments):\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tif comment.attrs['class']:\n",
    "\t\t\t\t\t\tif 'odd' in comment.attrs['class'] or 'even' in comment.attrs['class']:\n",
    "\t\t\t\t\t\t\tsingle_comment = get_single_comment(comment, True)\n",
    "\t\t\t\t\t\t\tall_comments.append(single_comment)\n",
    "\n",
    "\t\t\t\t# likely a comment thread\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tif comment.findChild('ol', class_=\"thread\"):\n",
    "\t\t\t\t\t\tall_comments.append(get_comment_thread(comment.findChildren('ol'), all_comments))\n",
    "\n",
    "\t\t\t# next page\n",
    "\t\t\tcount+=1\n",
    "\t\t\treq = requests.get(url+'?page='+str(count), headers=headers)\n",
    "\t\t\tsrc = req.text\n",
    "\t\t\tsoup = BeautifulSoup(src, 'html.parser')\n",
    "\t\t\t\n",
    "\telse:\n",
    "\t\tcomments = soup.find('ol', class_ = 'thread').findChildren(\"li\" , recursive=False)\n",
    "\t\tlast_count = 0\n",
    "\n",
    "\t\t# comments processing\n",
    "\t\tfor c, comment in enumerate(comments):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tif comment.attrs['class']:\n",
    "\t\t\t\t\tif 'odd' in comment.attrs['class'] or 'even' in comment.attrs['class']:\n",
    "\t\t\t\t\t\tsingle_comment = get_single_comment(comment, True)\n",
    "\t\t\t\t\t\tall_comments.append(single_comment)\n",
    "\t\t\t\t\t\tlast_count+=1\n",
    "\n",
    "\t\t\t# likely a comment thread\n",
    "\t\t\texcept:\n",
    "\t\t\t\tif comment.findChild('ol', class_=\"thread\"):\n",
    "\t\t\t\t\tif len(all_comments) > 0:\n",
    "\t\t\t\t\t\tnew_comments = get_comment_thread(comment.findChildren('ol'), True)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t# If it's a bunch of replies, store as reply to previous comment\n",
    "\t\t\t\t\t\tif len(all_comments) == last_count:\n",
    "\t\t\t\t\t\t\tall_comments[-1]['reply'] = new_comments\n",
    "\t\t\t\t\t\t\tlast_count+=1 # To move the index along so that you don't erase the comments\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tall_comments.append(new_comments)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tnew_comments = get_comment_thread(comment.findChildren('ol'), True)\n",
    "\t\t\t\t\t\tall_comments.append(new_comments)\n",
    "\t\t\t\t\t\tlast_count +=1\n",
    "\n",
    "\treturn all_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3ea624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = get_comments(\"https://archiveofourown.org/works/10057010/chapters/22409387?show_comments=true#comments\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7aa35de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1350"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "109f06bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user': '\\n',\n",
       " 'datetime': {'day': 'Mon',\n",
       "  'date': '04',\n",
       "  'month': 'Sep',\n",
       "  'year': '2023',\n",
       "  'time': '01:50PM',\n",
       "  'timezone': 'UTC'},\n",
       " 'id': '688155103',\n",
       " 'parent': False,\n",
       " 'text': 'Atyd is the one with the most hits???? Fr???? I didnâ€™t know that'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282fee87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
